# Photo Near-Duplicate Detection (macOS Photos)

Finds groups of near-duplicate photos using OpenCLIP embeddings, then lets you review them in a local web UI and optionally delete selected photos from Apple Photos.

**ğŸ“š Documentation:**
- **[PLAN.md](PLAN.md)** â€” 6-step infrastructure learning plan (all steps complete âœ…)
- **[DOCKER_README.md](DOCKER_README.md)** â€” Step 2: Docker containerization guide with security best practices
- **[GPU_DEPLOYMENT.md](GPU_DEPLOYMENT.md)** â€” Step 3: Cloud GPU deployment guide (Vast.ai, RunPod, Lambda Labs)
- **[TRITON_SETUP.md](TRITON_SETUP.md)** â€” Step 4: Triton Inference Server setup, benchmarking, and trade-offs
- **[STEP_5A_FINDINGS.md](STEP_5A_FINDINGS.md)** â€” Step 5A: ONNX Runtime optimization report (binary protocol fix, profiling)
- **[STEP_5B_TENSORRT.md](STEP_5B_TENSORRT.md)** â€” Step 5B: TensorRT EP setup guide and deployment
- **[STEP_5B_GPU_RESULTS.md](STEP_5B_GPU_RESULTS.md)** â€” Step 5B: TensorRT vs ONNX benchmark results
- **[STEP_6A_A100_RESULTS.md](STEP_6A_A100_RESULTS.md)** â€” Step 6A: 3-way backend comparison on A100 SXM4
- **[STEP_6A_RTX4080_RESULTS.md](STEP_6A_RTX4080_RESULTS.md)** â€” Step 6A: RTX 4080 comparison (consumer vs datacenter GPU)
- **[STEP_6B_RESULTS.md](STEP_6B_RESULTS.md)** â€” Step 6B: 4x RTX 4080 multi-GPU scaling study
- **[DEMO_SETUP_CLEAN.md](DEMO_SETUP_CLEAN.md)** â€” Demo mode setup (separate server on port 8001)
- **[docs/IMPLEMENTATION_NOTES.md](docs/IMPLEMENTATION_NOTES.md)** â€” Project history and implementation notes

**ğŸ¬ Project Presentation:**
- **[presentation/README.md](presentation/README.md)** â€” Complete presentation materials (slides, executive summary, technical deep-dive)
- **[presentation/slides.html](presentation/slides.html)** â€” Interactive HTML slide deck (18 slides, open in browser)

---

## ğŸ“Š Key Learnings: Backend Comparison Summary

After benchmarking 3 inference backends (PyTorch, Triton ONNX, Triton TensorRT) across multiple GPUs, here are the key takeaways:

### Performance by Backend

| Backend | Best For | GPU Compute (A100) | Client Latency (A100) | When to Use |
|---------|----------|:------------------:|:---------------------:|-------------|
| **PyTorch FastAPI** | Remote clients | 56.9ms | **56.9ms** âš¡ | Client-facing API (accepts JPEG) |
| **Triton ONNX CUDA EP** | Server-side | **4.4ms** âš¡ | 182.9ms | Local inference, batch processing |
| **Triton TensorRT EP** | Consumer GPUs | 2.0ms (RTX 4080) | 336ms | RTX GPUs, max GPU efficiency |

### Top 5 Insights

1. **Protocol Design > Compute Optimization**: PyTorch wins for remote clients (56.9ms) despite Triton being 12.8Ã— faster at GPU compute (4.4ms) â€” because PyTorch accepts 10KB JPEGs while Triton requires 602KB float tensors.

2. **GPU Architecture Matters**: TensorRT is 2.9Ã— faster on RTX 4080 but 6.5Ã— slower on A100. Consumer vs datacenter GPUs have opposite optimization profiles.

3. **Serialization Bottlenecks Are Real**: JSON `.tolist()` was 1,000â€“4,800Ã— slower than binary encoding, masking all GPU optimizations. Always profile the full pipeline.

4. **Multi-GPU Scaling Isn't Free**: 4Ã— RTX 4080 delivered only 1.8Ã— throughput (45% efficiency) â€” network/CPU became the bottleneck, not GPU.

5. **Production Recommendation**: Use PyTorch FastAPI for client-facing APIs + Triton ONNX as a local inference backend for batch workloads.

**ğŸ“ˆ Full benchmark data:** [presentation/04_BENCHMARK_RESULTS.md](presentation/04_BENCHMARK_RESULTS.md)

---

## What It Does

- **Scans photos** from Apple Photos (via AppleScript) or any directory
- **Detects duplicates** at three levels:
  - Exact duplicates (MD5 hash matching)
  - Perceptual duplicates (dHash - finds resized/edited versions)
  - AI similar groups (OpenCLIP embeddings - finds semantically similar photos)
- **Review UI** with separate workflows:
  - **Exact Duplicates**: Auto-selected by default, batch delete with one click
  - **Perceptual Duplicates**: Manual review, check groups to activate
  - **AI Groups**: Fine-grained selection, teach the AI with feedback
- **Smart features**:
  - Keeps oldest photo by default (based on EXIF date)
  - Shows disk savings in real-time
  - Full-screen modal with keyboard navigation
  - Individual image toggle within groups

## Quickstart

See DEMO_SETUP.md and DEMO_SOLUTION.md for demo

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Start UI only (port 8000)
python -m src.ui.main

# Or start both inference + UI
python start_services.py

# Or use demo dataset (UI on port 8001; inference runs on port 8002)
python start_services.py --ui-demo
```

Open http://127.0.0.1:8000 (or the demo UI at http://127.0.0.1:8001). The inference service runs on http://127.0.0.1:8002.

## End-to-End Workflow (How the Pieces Fit)

### Step 1 â€” Scan

Produces a JSON list of images to analyze.

- Output: scan_for_embeddings.json
- UI: Use the â€œControl Panel â†’ Start Scanâ€ button
- CLI (optional):

```bash
source venv/bin/activate
python -m src.scanner.main --photos-library --use-applescript --keep-export --limit 200
```

Tip: With `--keep-export`, AppleScript exports into `.cache/photos_export` by default and subsequent scans can be incremental (it skips re-exporting files that are already present).

macOS will prompt â€œTerminal wants to control Photosâ€ on first run. Allow it.

If you want estimates before running a full scan of your Photos â€œoriginalsâ€ (Full Disk Access required), run:

```bash
source venv/bin/activate
python -m src.scanner.main --photos-library --estimate --estimate-sample 200
```

### Step 2 â€” Generate Embeddings + Similarity Groups

Computes embeddings and writes similarity groups to disk.

- Output directory: embeddings/
- Group output: embeddings/similar_groups.json
- UI: Use â€œControl Panel â†’ Generate Embeddingsâ€ and set your threshold
- CLI (optional):

```bash
source venv/bin/activate
python -m src.embedding.main scan_for_embeddings.json --output embeddings --similarity-threshold 0.85
```

Notes:
- Higher thresholds (e.g., 0.95â€“0.99) are stricter.
- Embeddings are cleared before regenerating so old data doesnâ€™t accumulate.

### Step 3 â€” Review Groups (Delete / Keep / Feedback)

The UI has three tabs:

**Scanner Results Tab:**
- **Exact Duplicates** (green theme): All groups checked by default, oldest photo kept
  - Uncheck groups to exclude from batch delete
  - Click thumbnails to toggle individual images (KEEP â˜… / DEL âœ—)
  - Click Delete Selected to remove marked photos
- **Perceptual Duplicates** (orange theme): Manual review workflow
  - Check groups to activate them
  - Same toggle functionality as exact duplicates
  - Review and delete in batches

**AI Results Tab:**
- Side-by-side comparison of similar groups
- Delete Selected: deletes chosen photos from Apple Photos
- Keep All: marks the group reviewed
- Not Similar (Teach AI): stores "negative examples" in embeddings/feedback.pkl

**Universal Features:**
- ğŸ” Click to open full-screen modal with prev/next navigation
- Real-time disk savings calculation
- Keyboard shortcuts in modal (â† â†’ arrows, ESC to close)

### Step 4 â€” Re-analyze With Feedback

After giving feedback, click â€œRe-analyze with Feedbackâ€ (or regenerate embeddings) to apply the learned penalties/boosts during grouping.

## Outputs

- scan_for_embeddings.json â€” scan results used for embedding generation
- embeddings/embeddings.npy + embeddings/metadata.json â€” stored embeddings
- embeddings/similar_groups.json â€” groups to review
- embeddings/feedback.pkl â€” persisted feedback examples

## Architecture

The app uses a **client-service architecture** â€” the same pattern used by production inference systems like Triton, TorchServe, and vLLM.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client/UI          â”‚         â”‚ Inference Service    â”‚
â”‚  (Port 8000)         â”‚â”€HTTPâ”€â”€â†’ â”‚  (Port 8002)         â”‚
â”‚                      â”‚         â”‚                      â”‚
â”‚ â€¢ Scan photos       â”‚         â”‚ â€¢ Load model         â”‚
â”‚ â€¢ Call service      â”‚         â”‚ â€¢ Generate embeddingsâ”‚
â”‚ â€¢ Store embeddings  â”‚         â”‚ â€¢ Return JSON        â”‚
â”‚ â€¢ Group results     â”‚         â”‚ â€¢ Stateless API      â”‚
â”‚ â€¢ Display UI        â”‚         â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Inference Backends (switchable via env vars):**

| Backend | Docker Image | Ports | Status |
|---------|-------------|-------|--------|
| PyTorch + FastAPI | `Dockerfile.gpu` | 8002 | âœ… Step 3 |
| NVIDIA Triton (ONNX CUDA EP) | `Dockerfile.triton` | 8003 (HTTP), 8004 (gRPC), 8005 (metrics) | âœ… Step 4/5A |
| NVIDIA Triton (TensorRT EP) | `Dockerfile.tensorrt` | 8003 (HTTP), 8004 (gRPC), 8005 (metrics) | âœ… Step 5B |
| All 3 backends (unified) | `Dockerfile.step6a-all` | 8002 (PyTorch), 8003/8004/8005 (Triton) | âœ… Step 6A |

```bash
# Switch backends with environment variables:
export INFERENCE_BACKEND=pytorch   # or "triton"
export INFERENCE_SERVICE_URL=http://localhost:8002  # or :8003 for Triton
python -m src.ui.main
```

See [TRITON_SETUP.md](TRITON_SETUP.md) for Triton setup, benchmarking, and trade-offs evaluation.

**Key Principles:**
- **Separation of Concerns**: Client handles photos/metadata, service handles ML inference
- **Stateless Service**: Each request is independent, enabling horizontal scaling
- **Clean HTTP Boundary**: Services communicate via JSON/REST APIs
- **Independent Deployment**: Can run on same machine or separate GPU server

**Three Embedding Modes:**
```bash
# Local mode (original behavior - model loads inline)
python -m src.embedding.main_v2 scan_for_embeddings.json --mode local

# Remote mode (calls inference service)
python -m src.embedding.main_v2 scan_for_embeddings.json --mode remote

# Auto mode (tries remote, falls back to local)
python -m src.embedding.main_v2 scan_for_embeddings.json
```

See [DOCKER_README.md](DOCKER_README.md) for containerization details.

## Repo Map

- src/scanner/ â€” scanning + AppleScript Photos export
- src/embedding/ â€” OpenCLIP embedding generation + storage
- src/inference_service/ â€” stateless FastAPI inference server (client + server)
- src/grouping/ â€” clustering + feedback learner
- src/ui/ â€” FastAPI UI server
- scripts/ â€” benchmarking, profiling, deployment, and analysis tools
- model_repository/ â€” Triton model configs (ONNX CUDA EP + TensorRT EP)
- benchmark_results/ â€” raw benchmark JSON data from all steps

## Safety Notes

- Everything runs locally by default.
- Deletion uses AppleScript to delete photos from the Photos library; start by testing on a small limit.



Step	Time	Resource	Bottleneck
1. Scanner	1-5 min	I/O + CPU	File reading, hash computation
2. Embedding	10-30 sec	GPU/MPS	Neural network (MAIN BOTTLENECK)
3. Grouping	<1 sec	CPU	Just math on vectors
4. UI	Instant	Memory	Display