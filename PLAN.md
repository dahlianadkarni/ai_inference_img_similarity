
Step 1 (Most important): Split your app into "client" and "inference service" - âœ… DONE

Step 2: Containerize the inference service - âœ… DONE
- Create a Dockerfile for the inference service âœ…
- Expose:
    - /embed âœ…
    - /healthz âœ…
- Make model choice configurable via env vars âœ…
- Run it locally via Docker âœ…
- See DOCKER_STEP2.md for details

Step 3: Move the same container to NVIDIA GPU - âœ… DONE
- Run the container on:
    - Cloud GPU VM (T4 / A10 / RTX PRO 6000) âœ…
    - OR internal Akamai GPU if available
- Use:
    - NVIDIA container runtime âœ…
    - CUDA-enabled base image âœ…
- See GPU_DEPLOYMENT.md for details

Step 4: Introduce one inference framework. Eg: NVIDIA Triton Inference Server - âœ… DONE
- Serve your CLIP/OpenCLIP model via ONNX on Triton âœ…
- Use dynamic batching âœ…
- Call it via HTTP or gRPC âœ…
- Compare Triton vs "plain Python service" âœ…
- Evaluate trade-offs: model loading latency, dynamic batching, GPU utilization âœ…
- See TRITON_SETUP.md for details


Step 5A: ONNX Runtime/Triton Optimization (No TensorRT) - âœ… COMPLETE
    - Enable and test CUDA graphs in Triton config for GPU inference âœ…
    - Profile ONNX Runtime for slow ops or CPU fallbacks (use ONNX Runtime profiling tools) âœ…
    - Experiment with `max_queue_delay_microseconds` and client concurrency to maximize dynamic batching (document impact on throughput/latency) âœ…
    - Document any serialization/deserialization overheads found in ONNX Runtime âœ…
    - Observe and record GPU memory usage for ONNX Runtime and PyTorch backends âœ…
    - Summarize findings and update documentation with recommendations âœ…
    - GPU validation complete: Binary protocol fix confirmed, but native PyTorch outperforms ONNX by 5.2x for batch inference âœ…
    - See STEP_5A_FINDINGS.md for complete report âœ…
    - **Recommendation**: Continue with PyTorch backend (superior performance) or proceed to Step 5B (TensorRT) if Triton features are required


Step 5B (Optional): TensorRT Optimization - âœ… COMPLETE
    - Convert your model to TensorRT (if compatible) âœ… (pivoted to TRT Execution Provider â€” uses TRT 8.6 built into Triton)
    - Serve the TensorRT-optimized model via Triton âœ… (Dockerfile.tensorrt with TRT EP config, FP16)
    - Compare latency and throughput vs PyTorch and ONNX Runtime âœ… (scripts/trt_quick_test.py with server-side metrics)
    - Observe and record GPU memory usage for all backends âœ…
    - Document any serialization/deserialization overheads found in TensorRT backend âœ…
    - Summarize findings and update documentation with recommendations âœ…
    - **Result**: TRT EP achieves **6.6x faster GPU compute** (4.7ms vs 31.1ms) over ONNX CUDA EP
    - **Key finding**: TRT EP likely matches or exceeds native PyTorch (~10-15ms estimated), closing the 5.2x gap from Step 5A
    - See STEP_5B_TENSORRT.md for setup guide, STEP_5B_GPU_RESULTS.md for benchmark results

Step 6: Final Comparison & Multi-GPU Scaling (Optional)

**Step 6A: Single-Machine 3-Way Comparison** - âœ… COMPLETE
    - Deployed all 3 backends on **A100 SXM4 80GB** (Vast.ai) for apples-to-apples comparison:
        - PyTorch FastAPI (Step 3) âœ…
        - Triton ONNX CUDA EP (Step 4/5A) âœ…
        - Triton ONNX TensorRT EP (Step 5B) âœ…
    - **Results (30 iterations, remote benchmark):**
        - **PyTorch wins client-side:** 56.9ms single-image, 64.3 img/s batch-32
        - **Triton ONNX wins server-side:** 4.4ms GPU compute (12.8Ã— faster than PyTorch estimate)
        - **TRT EP not recommended:** 29.1ms GPU compute (6.5Ã— slower than ONNX), engine recompilation issues
    - **Key insight:** Network/serialization overhead dominates remote benchmarks.
      Triton ONNX achieves 4.4ms GPU compute but 182.9ms client-side (due to 602KB tensor transfer).
      PyTorch achieves 56.9ms client-side by accepting efficient base64 JPEG (~10KB).
    - **Production recommendation:** PyTorch FastAPI for client-facing API + Triton ONNX as local inference backend
    - See `STEP_6A_A100_RESULTS.md` for full analysis, `benchmark_results/step6a_a100_remote.json` for raw data

**Step 6B: Multi-GPU Scaling Study** - âœ… COMPLETE
    - Tested 4x RTX 4080 multi-GPU scaling on Vast.ai âœ…
    - **Results:**
        - **Peak throughput:** 43.2 img/s at concurrency 32 (1.8Ã— vs single GPU, only 45% efficiency) âœ…
        - **Scaling inefficiency:** Network overhead dominates (97% of latency), GPU compute only 8.4ms âœ…
        - **Cost-efficiency:** 2.2Ã— more expensive per image than 1x GPU âœ…
        - **Bottlenecks:** 602KB tensor payload, PCIe Gen4 interconnect, low dynamic batching (22%) âœ…
        - **Saturation point:** Concurrency 32; throughput drops at 64+ due to queue time buildup âœ…
    - **Key finding:** Multi-GPU is **not recommended** for remote inference due to network bottleneck
    - **Recommendation:** Use 1x A100 or horizontal scaling (3-4Ã— single-GPU instances) for production
    - **Optimization paths:** JPEG base64 input (5-6Ã— faster), local deployment (14Ã— faster), NVLink GPUs (2-3Ã— better scaling)
    - See `STEP_6B_RESULTS.md` for complete analysis
    - **Value**: Proves that multi-GPU doesn't solve network-bound workloads; validated infrastructure decision-making

**Step 7: 5-Way Protocol Comparison with gRPC** - âœ… COMPLETE
    - Motivation: gRPC (HTTP/2 + protobuf) should reduce per-call overhead vs HTTP/1.1 for the 602KB tensor payload bottleneck identified in Step 6A
    - **5 backends compared on same GPU instance (step6a docker-compose):**
        1. PyTorch FastAPI â€” HTTP/1.1 + base64 JPEG (~10KB/image)
        2. Triton ONNX HTTP â€” HTTP/1.1 + binary FP32 (~602KB/image)
        3. Triton ONNX gRPC â€” HTTP/2 + binary FP32 (~602KB/image)
        4. Triton TRT  HTTP â€” HTTP/1.1 + binary FP32 (~602KB/image)
        5. Triton TRT  gRPC â€” HTTP/2 + binary FP32 (~602KB/image)
    - **Why 5-way?** GPU and TRT vs ONNX differences are already known from Step 6A.
      The new question is: within each Triton backend, does gRPC reduce the 178ms transport overhead?
      Including PyTorch gives a reference point for what an optimized input format can achieve.
    - **Client changes:**
        - Added `triton_grpc` backend to `src/inference_service/client.py` using `tritonclient.grpc`
        - gRPC URL auto-derived as HTTP_PORT+1 (e.g. 8003 HTTP â†’ 8004 gRPC), or set via `TRITON_GRPC_URL` env var
    - **Benchmark script:** `scripts/benchmark_grpc_vs_http.py`
        - All 5 backends benchmarked on the same Vast.ai instance
        - Batch sizes: 1, 4, 8, 16, 32  |  Concurrency: 1, 8, 16
        - Records p50/p95/p99 latency, img/s throughput, server-side GPU compute from Triton metrics
        - Saves raw JSON to `benchmark_results/step7_5way_<timestamp>.json`
    - **Expected findings to measure:**
        - gRPC vs HTTP overhead isolation (same GPU compute, different transport)
        - Whether HTTP/2 multiplexing improves concurrent throughput
        - PyTorch JPEG input as lower bound on client-side latency (10KB vs 602KB)
    - **Requirements:** `tritonclient[http,grpc]>=2.41.0` (updated in requirements.txt)
    - **Test instance:** Vast.ai A100 SXM4 80GB â€” IP `207.180.148.74` (Massachusetts, USA) â€” Instance ID 31781954
    - **Port mappings:** PyTorch 47150, ONNX HTTP 47088, ONNX gRPC 47037, TRT HTTP 47008, TRT gRPC 47045
    - **Results (30 iterations, A100 SXM4 80GB, 2026-02-20):**
        - **PyTorch wins outright:** 64.2ms batch-1, 48.5 img/s batch-32 â€” 3Ã— faster than best Triton serial
        - **gRPC is slower for batch=1:** 0.86â€“0.96Ã— vs HTTP; TCP handshake is NOT the bottleneck
        - **gRPC wins from batch=4+:** 1.4â€“1.7Ã— speedup for ONNX; HTTP/2 framing pays off at multi-MB sizes
        - **HTTP scales better concurrently:** ONNX HTTP 43.6 img/s at conc=16 vs ONNX gRPC 7.5 img/s (client channel contention)
        - **TRT engine anomaly:** TRT HTTP shows ~1658ms GPU metric (engine compiling); TRT gRPC (post-cache) = 3.6ms
    - **Key finding:** gRPC does NOT reduce the 178ms transport overhead at batch=1. The bottleneck is 602KB payload bandwidth. Input format (JPEG base64) is the only path to <100ms remote latency.
    - Raw data: `benchmark_results/step7_5way_20260220_221313.json`
    - See `STEP_7_GRPC_RESULTS_A100.md` for full A100 analysis

**Step 7B: RTX 4090 Repeat â€” Pennsylvania** - âœ… COMPLETE
    - Repeating 5-way benchmark on consumer-grade RTX 4090 to compare against A100
    - Slower GPU compute â†’ transport overhead is a smaller fraction of total latency â€” different gRPC tradeoff expected
    - **Test instance:** Vast.ai RTX 4090 â€” IP `173.185.79.174` (Pennsylvania, USA) â€” $0.391/hr
    - **Port mappings:** PyTorch 50616, ONNX HTTP 50680, ONNX gRPC 50764, TRT HTTP 50048, TRT gRPC 50286
    - **Results (30 iterations, RTX 4090, 2026-02-20):**
        - **PyTorch concurrent peak:** 49.1 img/s at conc=16 (vs 30.5 on A100) â€” FastAPI async + JPEG scales well
        - **gRPC worse at batch=1 than A100:** 11â€“17% slower vs HTTP (vs 4â€“14% on A100); H5 confirmed
        - **gRPC ONNX wins at batch=8+:** 2.0Ã— speedup (only from b=8 vs b=4 on A100); slower GPU shifts crossover point
        - **HTTP still wins under concurrency:** ONNX HTTP 32.2 img/s vs ONNX gRPC 4.0 img/s at conc=16 (8Ã— gap vs 5.8Ã— on A100)
        - **Cost-efficiency:** RTX 4090 latency is 1.3â€“2.1Ã— worse than A100 at 2.3Ã— lower cost â€” broadly fair
    - Raw data: `benchmark_results/step7_5way_20260220_232454.json`
    - See `STEP_7_GRPC_RESULTS_RTX_4090.md` for full analysis with cross-GPU comparison

**Step 8: Local Kubernetes (kind)** - ðŸ”„ IN PROGRESS
    - Goal: deploy the same inference container into a local Kubernetes cluster (kind) to learn K8s orchestration patterns
    - **Tool**: kind (Kubernetes-in-Docker) + kubectl + metrics-server + hey load generator
    - **Image**: `photo-duplicate-inference:k8s-cpu` â€” same `Dockerfile`, new tag, ARM64-native, never pushed to Docker Hub
    - **Port**: `localhost:8092` (NodePort 30092 via kind extraPortMappings â€” avoids collision with existing 8002/8003/8004)
    - **Phase 1 âœ…**: kind v0.31.0 cluster created, metrics-server installed + patched for kind TLS, image built and loaded
    - **Phase 2 âœ…**: All manifests applied (`kubectl apply -k k8s/`); 2/2 pods Running; `GET /health â†’ {"status":"ok"}`
    - **Phase 3 âœ…**: HPA deployed (2â€“6 replicas, 60% CPU target, 3-min scaledown stabilization)
    - **Phase 4**: kubectl observability practice (logs, exec, top, rolling update, rollback) â€” pending
    - **Phase 5**: PodDisruptionBudget + ResourceQuota â€” deployed alongside Phase 2 âœ…
    - **Phase 6** (optional): Helm chart for multi-environment deploy
    - See `K8S_PLAN.md` for full phased plan and coexistence notes
    - See `STEP_8_K8S_RESULTS.md` for live cluster output