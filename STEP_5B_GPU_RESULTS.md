# Step 5B: TensorRT EP Benchmark Results

**Date**: February 15, 2026  
**Instance**: Vast.ai RTX A4000 (16GB VRAM), Instance 31467604  
**Model**: OpenCLIP ViT-B-32 (336MB ONNX)  
**Server**: Triton Inference Server 24.01 (CUDA 12.3, TRT 8.6 built-in)  
**Approach**: ONNX Runtime with TensorRT Execution Provider (FP16)

---

## Quick Results

### Winner: TensorRT EP ðŸ† (6.6x faster GPU compute)

| Metric | ONNX (CUDA EP) | TRT EP (FP16) | Speedup |
|--------|---------------|---------------|---------|
| **Server-side GPU compute** | 31.1ms | **4.7ms** | **6.6x** |
| Server-side request | 41.9ms | 15.9ms | 2.6x |
| Client latency (over internet) | 359ms | 454ms* | - |

*\*Client latency is dominated by ~200-400ms network round-trip from Mac â†’ Vast.ai.*

### Key Insight

Server-side metrics (from Triton `/metrics` endpoint) reveal the true GPU performance, stripped of network overhead. The TensorRT Execution Provider with FP16 delivers a **6.6x speedup** in pure GPU compute time over standard ONNX Runtime CUDA EP.

---

## Architecture Change from Original Plan

### What We Planned (Standalone TRT .plan)
The original plan was to convert the ONNX model to a standalone `.plan` file using `trtexec`. This failed because:

1. **`trtexec` not included** in the Triton container image
2. **pip TensorRT 10.15 incompatible** with Triton 24.01's CUDA 12.3 (`cudaErrorInsufficientDriver`)

### What Worked (TRT Execution Provider)
We pivoted to using ONNX Runtime's **TensorRT Execution Provider**, which:

- Uses TRT 8.6 libraries **already bundled** in the Triton container
- No separate conversion step â€” TRT EP automatically compiles supported ops
- Caches compiled TRT engines in `/tmp/trt_cache` (instant on subsequent loads)
- First load takes ~2-5 minutes for TRT engine compilation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Triton Inference Server 24.01               â”‚
â”‚                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚   â”‚ openclip_vit_b32 â”‚  â”‚ openclip_vit_b32_trt     â”‚â”‚
â”‚   â”‚  ONNX Runtime    â”‚  â”‚  ONNX Runtime            â”‚â”‚
â”‚   â”‚  CUDA EP (FP32)  â”‚  â”‚  TensorRT EP (FP16)      â”‚â”‚
â”‚   â”‚  Compute: 31ms   â”‚  â”‚  Compute: 4.7ms          â”‚â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                      â”‚
â”‚   Same ONNX model file (symlinked), different EPs    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Results

### Server-Side Metrics (No Network Overhead)

These numbers come directly from Triton's Prometheus `/metrics` endpoint, measuring only the server-side processing time:

| Metric | ONNX (CUDA EP) | TRT EP (FP16) | Speedup |
|--------|---------------|---------------|---------|
| Avg compute time | 31.1ms | **4.7ms** | **6.6x** |
| Avg request time | 41.9ms | 15.9ms | 2.6x |
| Avg input processing | 1.2ms | 0.5ms | 2.4x |

The gap between "compute" and "request" time includes:
- Input deserialization / tensor copy to GPU
- Output serialization / tensor copy from GPU
- Request queuing overhead

### Client-Side Latency (Includes Network)

Measured from MacBook Pro (Boston) â†’ Vast.ai (datacenter), single-image [1,3,224,224]:

| Metric | ONNX (CUDA EP) | TRT EP (FP16) |
|--------|---------------|---------------|
| Mean | 359ms | 454ms |
| p50 | 339ms | 454ms |
| Min | 210ms | 241ms |

Client latency is **network-dominated** (~200-400ms round-trip). The actual inference improvement is masked by the network overhead. In a co-located deployment (client on same network), TRT EP would show clear client-side wins.

### Batch-4 Server-Side (from metrics delta)

| Metric | ONNX (CUDA EP) |
|--------|---------------|
| Avg compute | 31.0ms |
| Avg request | 33.1ms |

*TRT EP batch-4 data not captured due to slow network transfers for batch payloads (4Ã—3Ã—224Ã—224 = 2.4MB per request).*

---

## Comparison Across All Steps

### Server-Side GPU Compute (Single Image)

| Backend | Compute Time | vs ONNX | Notes |
|---------|-------------|---------|-------|
| ONNX Runtime (CUDA EP) | 31.1ms | baseline | FP32, standard EP |
| **TRT EP (FP16)** | **4.7ms** | **6.6x faster** | FP16, kernel fusion |
| PyTorch (native) | ~10-15ms* | ~2-3x faster | Estimated from Step 5A client data |

*\*PyTorch server-side compute estimated from Step 5A client latency (189ms mean) minus network overhead (~175ms).*

### End-to-End Client Latency (Remote over Internet)

| Backend | Client Latency | Source |
|---------|---------------|--------|
| PyTorch (Step 5A) | 189ms mean | Direct FastAPI |
| ONNX Runtime (Step 5A) | 319ms mean | Triton + binary protocol |
| ONNX Runtime (Step 5B) | 359ms mean | Same setup, different day |
| TRT EP (Step 5B) | 454ms mean* | First runs, high variance |

*\*Higher client latency for TRT EP is due to network variance, not GPU performance (server compute is 4.7ms vs 31ms).*

---

## Why TRT EP Is So Much Faster

1. **FP16 Precision**: Halves memory bandwidth requirements. ViT-B-32 embeddings have >0.999 cosine similarity between FP32 and FP16.

2. **Kernel Fusion**: TRT fuses sequences of operations (LayerNorm + MatMul + GELU â†’ single kernel), eliminating intermediate memory reads/writes.

3. **Architecture-Specific Tuning**: TRT auto-tunes kernel implementations for the specific GPU (RTX A4000 / Ampere architecture).

4. **Memory Layout Optimization**: TRT reorders tensor memory layouts for optimal GPU cache utilization.

---

## Deployment Notes

### Docker Image
- **Image**: `dahlianadkarni/photo-duplicate-triton:tensorrt-gpu`
- **Base**: `nvcr.io/nvidia/tritonserver:24.01-py3`
- **Build time**: ~58 seconds (no heavy pip installs)
- **First start**: 2-5 minutes (TRT engine compilation)
- **Subsequent starts**: ~30 seconds (cached engine)

### Configuration
- TRT EP config in `model_repository/openclip_vit_b32_trt/config.pbtxt`
- FP16 precision with 4GB workspace
- Engine cache in `/tmp/trt_cache`

### Key Files
| File | Purpose |
|------|---------|
| `model_repository/openclip_vit_b32_trt/config.pbtxt` | TRT EP Triton config |
| `scripts/triton_trt_entrypoint.sh` | Container entrypoint (symlinks ONNX model) |
| `Dockerfile.tensorrt` | Docker image definition |
| `scripts/trt_quick_test.py` | Quick A/B benchmark with server metrics |
| `scripts/benchmark_tensorrt.py` | Full 3-way benchmark script |

---

## Recommendations

1. **Use TRT EP for production Triton deployments** â€” 6.6x GPU compute speedup makes Triton competitive with native PyTorch.

2. **TRT EP closes the PyTorch gap** â€” Step 5A showed ONNX was 5.2x slower than PyTorch. TRT EP's 4.7ms compute is likely **faster** than PyTorch's estimated 10-15ms.

3. **Network latency dominates remote benchmarks** â€” For accurate comparison, benchmark from within the same datacenter or use server-side metrics.

4. **First-load penalty is acceptable** â€” 2-5 minute TRT compilation on first load, then cached. Suitable for long-running inference servers.

5. **FP16 is safe for embeddings** â€” Cosine similarity >0.999 between FP16 and FP32 outputs. No accuracy loss for duplicate detection.
