# =============================================================================
# OpenCLIP ViT-B-32 — Triton Model Configuration (GPU-Optimized)
# =============================================================================
#
# This config is baked into the Docker image (Dockerfile.triton) so that
# every new Vast.ai instance starts with optimized GPU defaults — no manual
# config step needed.
#
# PROFILE: GPU (production default)
#   - kind: KIND_GPU with CUDA graphs enabled
#   - Optimized for RTX A4000 / RTX 3090 / A40 class GPUs (8-24 GB VRAM)
#
# FOR LOCAL CPU TESTING:
#   Override at container start by bind-mounting a CPU config:
#     docker run -v $PWD/model_repository:/models ...
#   Or use build_triton_local.sh which generates a CPU config on the fly.
#
# DYNAMIC BATCHING RATIONALE:
#   preferred_batch_size: [4, 8, 16, 32]
#     - 4:  Low-latency for interactive single-user requests
#     - 8:  Good balance for small concurrent bursts
#     - 16: Efficient GPU utilization for moderate load
#     - 32: Maximum throughput under high concurrency (matches max_batch_size)
#
#   max_queue_delay_microseconds: 10000 (10ms)
#     - Triton waits up to 10ms to accumulate a larger batch before executing.
#     - 10ms is a good balance for mixed interactive + batch workloads.
#     - Tune down to 1-5ms for latency-sensitive interactive use,
#       or up to 50ms for batch-heavy workloads.
#
# max_batch_size: 32
#   - ViT-B-32 with FP32 at batch=32 uses ~2.5GB VRAM.
#   - Safe for 8GB+ GPUs. Increase to 64 on 24GB+ GPUs for more throughput.
#
# CUDA GRAPHS:
#   Enabled by default. Pre-captures GPU kernel launch sequences for
#   reduced launch overhead (~10-20% latency improvement on repeated shapes).
#
# =============================================================================

name: "openclip_vit_b32"
platform: "onnxruntime_onnx"
max_batch_size: 32

input [
  {
    name: "image"
    data_type: TYPE_FP32
    dims: [ 3, 224, 224 ]
  }
]

output [
  {
    name: "embedding"
    data_type: TYPE_FP32
    dims: [ 512 ]
  }
]

# Dynamic batching — Triton automatically groups concurrent requests
# into batches for better GPU utilization.
dynamic_batching {
  preferred_batch_size: [ 4, 8, 16, 32 ]
  max_queue_delay_microseconds: 10000
}

# Instance configuration — GPU (production default)
instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [ 0 ]
  }
]

# Version policy — only load version 1 explicitly
version_policy: { specific { versions: 1 }}

# CUDA graph optimization — reduces kernel launch overhead
optimization {
  cuda {
    graphs: true
  }
}
